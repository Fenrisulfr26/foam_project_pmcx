{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad8f55f4",
   "metadata": {},
   "source": [
    "## batch MCX simulation HF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df9c5a33",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _C: 找不到指定的程序。",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msignal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fftconvolve\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datasets, transforms\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[1;32mf:\\program_files1\\miniconda3\\envs\\diffusion\\lib\\site-packages\\torchvision\\__init__.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodulefinder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n",
      "File \u001b[1;32mf:\\program_files1\\miniconda3\\envs\\diffusion\\lib\\site-packages\\torch\\__init__.py:367\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[0;32m    366\u001b[0m         _load_global_deps()\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mSymInt\u001b[39;00m:\n\u001b[0;32m    371\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m    Like an int (including magic methods), but redirects all operations on the\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m    wrapped node. This is used in particular to symbolically record operations\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    in the symbolic shape workflow.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _C: 找不到指定的程序。"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Sep 25 19:50:49 2025\n",
    "\n",
    "@author: zhiguan wang\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.constants as const\n",
    "import pmcx\n",
    "from scipy.optimize import curve_fit\n",
    "from cv2 import resize\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import fftconvolve\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import torch\n",
    "import collections\n",
    "from torch.utils.data import DataLoader,Subset\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# create an output widget for logging\n",
    "disp_region = widgets.Output(layout={'border': '1px solid black', 'height': '800px', 'overflow_y': 'scroll'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca84f57",
   "metadata": {},
   "source": [
    "## prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cffa770",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"F:/OneDrive/foam_imaging_project/pmcx_foam/MNIST_img_dataset\"  # root fold for the data       \n",
    "SPLIT = \"byclass\"       \n",
    "NUM_TOTAL = 5000      \n",
    "RANDOM_SEED = 42       \n",
    "BATCH_SIZE = 64\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_ds = datasets.EMNIST(root=ROOT, split=SPLIT, train=True, download=True, transform=transform)\n",
    "test_ds  = datasets.EMNIST(root=ROOT, split=SPLIT, train=False, download=True, transform=transform)\n",
    "\n",
    "class ConcatDatasetLike(torch.utils.data.Dataset):\n",
    "    def __init__(self, ds1, ds2):\n",
    "        self.ds1 = ds1\n",
    "        self.ds2 = ds2\n",
    "        self.len1 = len(ds1)\n",
    "        self.len2 = len(ds2)\n",
    "    def __len__(self):\n",
    "        return self.len1 + self.len2\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < self.len1:\n",
    "            return self.ds1[idx]\n",
    "        else:\n",
    "            return self.ds2[idx - self.len1]\n",
    "\n",
    "full_ds = ConcatDatasetLike(train_ds, test_ds)\n",
    "\n",
    "# collect the index of every label and fetch the label of\n",
    "label_to_indices = collections.defaultdict(list)\n",
    "for idx in range(len(full_ds)):\n",
    "    _, label = full_ds[idx]\n",
    "    # EMNIST byclass: labels 0-61 \n",
    "    if 10 <= label <= 35:\n",
    "        label_to_indices[int(label)].append(idx)\n",
    "\n",
    "num_classes = 26\n",
    "base = NUM_TOTAL // num_classes            # 192\n",
    "remainder = NUM_TOTAL % num_classes       # 8\n",
    "per_class_target = {i: base + (1 if i < remainder else 0) for i in range(num_classes)}\n",
    "\n",
    "# random sampling\n",
    "g = torch.Generator().manual_seed(RANDOM_SEED-1)  # select new set of data\n",
    "selected_indices = []\n",
    "\n",
    "for i in range(num_classes):\n",
    "    label = 10 + i\n",
    "    available = label_to_indices.get(label, [])\n",
    "    want = per_class_target[i]\n",
    "    if len(available) >= want:\n",
    "        perm = torch.randperm(len(available), generator=g)\n",
    "        chosen_local = [available[int(perm[j])] for j in range(want)] # adding an index here to select another set of data\n",
    "        selected_indices.extend(chosen_local)\n",
    "    else:\n",
    "        print(f\"Warning: label {label} ({chr(ord('A')+i)}) has only {len(available)} samples, requested {want}. Using all available.\")\n",
    "        selected_indices.extend(available)\n",
    "\n",
    "print(\"Total selected:\", len(selected_indices))\n",
    "\n",
    "if len(selected_indices) > NUM_TOTAL:\n",
    "    selected_indices = selected_indices[:NUM_TOTAL]\n",
    "elif len(selected_indices) < NUM_TOTAL:\n",
    "    print(f\"Note: only {len(selected_indices)} samples were collected (<{NUM_TOTAL}). Consider lowering NUM_TOTAL or allowing imbalanced sampling.\")\n",
    "\n",
    "subset_ds = Subset(full_ds, selected_indices)\n",
    "\n",
    "loader = DataLoader(subset_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "# show numbers in each label\n",
    "count_by_label = collections.Counter()\n",
    "for idx in selected_indices:\n",
    "    _, label = full_ds[idx]\n",
    "    count_by_label[int(label)] += 1\n",
    "\n",
    "print(\"Per-class counts (label -> count) for A-Z:\")\n",
    "for i in range(num_classes):\n",
    "    lab = 10 + i\n",
    "    ch = chr(ord(\"A\") + i)\n",
    "    print(f\"{ch} (label {lab}): {count_by_label.get(lab,0)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72851f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% get the sensitivity map and the IRF\n",
    "data_fold = r'F:/OneDrive/UK_projects_local/project in UK 2024\\diffuse_experiment241021/zhiguan_data/XL_measurement_061224'\n",
    "\n",
    "#-----------------prepare the sensitibity map-------------\n",
    "sens_path = os.path.join(data_fold,r'sensitivity_map_gain0.7_8x8cm_51x51points_binWidth15ps_expo0.1sec_binNum2000_40deg_061224.npy')\n",
    "sensitivity = np.load(sens_path).sum(2)\n",
    "sensitivity = sensitivity/sensitivity.max()\n",
    "sensitivity = np.fliplr(sensitivity)\n",
    "sensitivity = resize(sensitivity, (80,80), interpolation=cv2.INTER_LINEAR)\n",
    "sens_pad = np.zeros((250,250))\n",
    "sens_pad[125-40:125+40, 125-40:125+40] = sensitivity\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(sens_pad)\n",
    "plt.show()\n",
    "\n",
    "#-----------------prepare the IRF---------------------\n",
    "IRF_path = os.path.join(data_fold,r'IRF_gain0.7_timebin15ps_2000bins_061224.npy')\n",
    "IRF = np.load(IRF_path)[:1000]\n",
    "IRF_down = resize(IRF[:np.round(121*100/15).astype('int')], (1, 121), interpolation=cv2.INTER_NEAREST_EXACT)\n",
    "IRF_down= np.squeeze(IRF_down)\n",
    "plt.plot(IRF_down)\n",
    "\n",
    "sim_len = 121\n",
    "\n",
    "src_x_positions = np.linspace(125+25,125-25,3) # from PMT view, from right to left\n",
    "src_y_positions = np.linspace(125+25,125-25,3) # from PMT view, from up to down\n",
    "\n",
    "#--------------MCX cfg------------------\n",
    "try:\n",
    "    cfg.clear()\n",
    "except NameError:\n",
    "    pass  \n",
    "    \n",
    "cfg = {'nphoton': 1e7, \n",
    "       'tstart':0, \n",
    "       \n",
    "        'tend':sim_len*1e-10, \n",
    "        'tstep':0.1e-9, # 100 ps resolution\n",
    "       'srcpos': [10,125,0], \n",
    "       'srcdir':[0,0,1],\n",
    "       'unitinmm':1,\n",
    "       # 'detpos': [125, 125, t_vox, 4], #radius approx 1mm (4vox) (fibre ~2mm from sample)\n",
    "       'issrcfrom0':1,\n",
    "       'issavedet':1,\n",
    "       'issaveref':1\n",
    "       }\n",
    "\n",
    "\n",
    "cfg['prop'] = [[0,0,1,1],           # background\n",
    "               [0.0019, 1.4800,0,1.44], # volume 1\n",
    "               ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf9a310",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'F:/OneDrive/foam_imaging_project/pmcx_foam/batch_mcx_sim_data/data_231125'\n",
    "gd_path = os.path.join(save_dir, 'ds_objs_50x50_5x5cm.npy')\n",
    "sims_result_path = os.path.join(save_dir, 'batch_mcx_sims_results.npy')\n",
    "checkpoint_json = os.path.join(save_dir, 'checkpoint.json')\n",
    "\n",
    "total_count = 5000\n",
    "save_interval = 100  # save every 100 sims\n",
    "\n",
    "start_index = 0\n",
    "\n",
    "# 1. check for existing checkpoint\n",
    "if os.path.exists(checkpoint_json) and os.path.exists(gd_path) and os.path.exists(sims_result_path):\n",
    "    print(\"found check point, reloading...\")\n",
    "    \n",
    "    # redad checkpoint\n",
    "    with open(checkpoint_json, 'r') as f:\n",
    "        ckpt = json.load(f)\n",
    "        start_index = ckpt['last_index'] + 1 # start from next index\n",
    "        \n",
    "    # load saved big data file (Loading to RAM)\n",
    "    ground_truths = np.load(gd_path)\n",
    "    sim_results = np.load(sims_result_path)\n",
    "    \n",
    "    print(f\"successful loading, start from {start_index} \")\n",
    "else:\n",
    "    print(\"no checkpoint, restart...\")\n",
    "    start_index = 0\n",
    "    ground_truths = np.zeros((50, 50, total_count), dtype=np.float32) # \n",
    "    sim_results = np.zeros((3, 3, sim_len, total_count), dtype=np.float32)\n",
    "\n",
    "# ================= main loop =================\n",
    "if start_index < total_count:\n",
    "\n",
    "    display(disp_region)\n",
    "    with disp_region:\n",
    "\n",
    "        # range 从 start_index 开始\n",
    "        for img_index in tqdm(range(start_index, total_count), initial=start_index, total=total_count):\n",
    "\n",
    "            # ---------------- start processing ----------------\n",
    "            img, lab = subset_ds[img_index]  \n",
    "\n",
    "            img = np.fliplr(np.rot90(img.squeeze(),k = -1,axes = (0,1))) \n",
    "            img = cv2.copyMakeBorder(\n",
    "                img, top=2, bottom=2, left=2, right=2,  \n",
    "                borderType=cv2.BORDER_CONSTANT, value=0   \n",
    "            )\n",
    "            \n",
    "            img = cv2.resize(img, (50, 50), interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "            threshold = 0\n",
    "            img[img<=threshold]=0\n",
    "            img[img>threshold]=1\n",
    "            img = 1-img\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.imshow(img, cmap='gray')\n",
    "            plt.show()\n",
    "\n",
    "            # save ground_truths\n",
    "            ground_truths[:,:,img_index] = img\n",
    "        \n",
    "            obj_in_mcx_PMT_padding = cv2.copyMakeBorder(\n",
    "                img, 100, 100, 100, 100, cv2.BORDER_CONSTANT, value=1\n",
    "            )\n",
    "            obj_in_mcx_PMT_padding = np.uint8(obj_in_mcx_PMT_padding)\n",
    "            \n",
    "            # MCX Volume setup\n",
    "            thickness = 50 \n",
    "            unitinmm = 1\n",
    "            t_vox = int(thickness/unitinmm)\n",
    "            vol = np.ones([250,250,t_vox+1],dtype='uint8')\n",
    "            vol[:,:,-1] = 0\n",
    "            vol[:,:,24] = np.rot90(obj_in_mcx_PMT_padding,k = -1,axes = (0,1)) \n",
    "            cfg['vol'] = vol\n",
    "\n",
    "            sim_mea = np.zeros((3,3,sim_len))\n",
    "            \n",
    "            # Scanning loop\n",
    "            for i, pos_y in enumerate(src_y_positions):\n",
    "                for j, pos_x in enumerate(src_x_positions):\n",
    "                    cfg['srcpos'] = [pos_x, pos_y, 0]\n",
    "                    res = pmcx.run(cfg)\n",
    "                    dref_bd = res['dref'][:,:,int(vol.shape[2]-1),:] \n",
    "                    dref_bd_rot = np.rot90(dref_bd, k=1, axes = (0,1)) \n",
    "                    sim_mea[i,j,:] = (dref_bd_rot*sens_pad[:,:,None]).sum((0,1)) \n",
    "                        \n",
    "            sim_mea = np.apply_along_axis(lambda m: fftconvolve(m, IRF_down, mode='full'), axis=2, arr=sim_mea)[:,:,:sim_len]\n",
    "            sim_mea = sim_mea/sim_mea.max()\n",
    "            \n",
    "            # 存入 sim_results\n",
    "            sim_results[:,:,:,img_index] = sim_mea\n",
    "            \n",
    "            # ---------------- Checkpoint save ----------------\n",
    "            # evert save_interval, or the last one\n",
    "            if (img_index + 1) % save_interval == 0 or (img_index + 1) == total_count:\n",
    "                print(f\"\\nCheckpoint: Saving data at index {img_index}...\")\n",
    "                \n",
    "                # 1. save big data files (NumPy)\n",
    "                np.save(gd_path, ground_truths)\n",
    "                np.save(sims_result_path, sim_results)\n",
    "                \n",
    "                # 2. save checkpoint (JSON)\n",
    "                with open(checkpoint_json, 'w') as f:\n",
    "                    json.dump({'last_index': img_index}, f)\n",
    "                    \n",
    "                print(\"Saved.\")\n",
    "\n",
    "print(\"All Done!\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
